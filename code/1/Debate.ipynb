{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea996e1a-fe5a-4da1-8dcf-d478652ef669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /localscratch-nvme/3179674/pip-req-build-xio1c0oq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /localscratch-nvme/3179674/pip-req-build-xio1c0oq\n",
      "  Resolved https://github.com/huggingface/transformers to commit e6a8063ef1af16df964b644b07e1d17e96555d23\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /home/apoorvachavali/.local/lib/python3.11/site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /home/apoorvachavali/.local/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from transformers==4.54.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.54.0.dev0) (1.1.5)\n",
      "Requirement already satisfied: psutil in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from triton==3.3.1->torch>=2.0.0->accelerate) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->transformers==4.54.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->transformers==4.54.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->transformers==4.54.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->transformers==4.54.0.dev0) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6769607b-a78a-4bb7-8710-4738c564ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/apoorvachavali/.local/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from triton==3.3.1->torch) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qwen_vl_utils in /home/apoorvachavali/.local/lib/python3.11/site-packages (0.0.11)\n",
      "Requirement already satisfied: av in /home/apoorvachavali/.local/lib/python3.11/site-packages (from qwen_vl_utils) (14.4.0)\n",
      "Requirement already satisfied: packaging in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from qwen_vl_utils) (25.0)\n",
      "Requirement already satisfied: pillow in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from qwen_vl_utils) (11.2.1)\n",
      "Requirement already satisfied: requests in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from qwen_vl_utils) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->qwen_vl_utils) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->qwen_vl_utils) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->qwen_vl_utils) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests->qwen_vl_utils) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e083f346-8a1d-4a25-8670-f5fcd4e46644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/apoorvachavali/.local/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->torchvision) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14969782-aa05-4a38-a441-a030bfc535a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/apoorvachavali/.local/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from tensorflow) (80.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/apoorvachavali/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /home/apoorvachavali/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/apoorvachavali/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /apps/common/software/Miniforge3/24.11.3-0-jupyter-base/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/apoorvachavali/.local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3411335c-eaf3-46f7-a5fc-df44b358d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 10:41:56.499752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751640116.943940 3925795 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751640117.054188 3925795 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751640118.138210 3925795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751640118.138232 3925795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751640118.138234 3925795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751640118.138236 3925795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-04 10:41:58.265730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU available: True\n",
      "Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow GPU available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "print(\"Devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34fb2ae6-e4d6-4495-a5d5-ed90483a2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    LlavaForConditionalGeneration, AutoProcessor as LlavaProcessor,\n",
    "    Qwen2_5_VLForConditionalGeneration, AutoProcessor as QwenProcessor\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8f7f95-3335-4033-b928-a4f32d0a152e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48fffd590fe43f8964b3be44b3278b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3eea37655d5412eafeed162857074d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "llava_processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "qwen_processor = QwenProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0985687c-9d8e-4c32-ab43-83dc75cc27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sqa = load_dataset('derek-thomas/ScienceQA', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225b6f69-0beb-43e1-a406-81620ef2132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"lmms-lab/ai2d\", split=\"test[:1000]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d370d2a6-b141-4290-b2b9-dd3ca1c33085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=760x506 at 0x1553C91EB690>, 'question': 'Which of these organisms contains matter that was once part of the lichen?', 'choices': ['bilberry', 'mushroom'], 'answer': 1, 'hint': 'Below is a food web from a tundra ecosystem in Nunavut, a territory in Northern Canada.\\nA food web models how the matter eaten by organisms moves through an ecosystem. The arrows in a food web represent how matter moves between organisms in an ecosystem.', 'task': 'closed choice', 'grade': 'grade5', 'subject': 'natural science', 'topic': 'biology', 'category': 'Ecosystems', 'skill': 'Interpret food webs II', 'lecture': 'A food web is a model.\\nA food web shows where organisms in an ecosystem get their food. Models can make things in nature easier to understand because models can represent complex things in a simpler way. If a food web showed every organism in an ecosystem, the food web would be hard to understand. So, each food web shows how some organisms in an ecosystem can get their food.\\nArrows show how matter moves.\\nA food web has arrows that point from one organism to another. Each arrow shows the direction that matter moves when one organism eats another organism. An arrow starts from the organism that is eaten. The arrow points to the organism that is doing the eating.\\nAn organism in a food web can have more than one arrow pointing from it. This shows that the organism is eaten by more than one other organism in the food web.\\nAn organism in a food web can also have more than one arrow pointing to it. This shows that the organism eats more than one other organism in the food web.', 'solution': 'Use the arrows to follow how matter moves through this food web. For each answer choice, try to find a path of arrows that starts from the lichen.\\nNo arrow points to the bilberry. So, in this food web, matter does not move from the lichen to the bilberry.'}\n"
     ]
    }
   ],
   "source": [
    "print(data_sqa[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f33ac50-874a-4370-b9b5-6d9d23192ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_qwen(data_point):\n",
    "    image = data_point['image']\n",
    "    if image is not None:\n",
    "        \n",
    "        text = str(\"question:\" + data_point['question'] + \"choices:\" + str(data_point['choices']))\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": data_point['image'],\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": text + \"What is your answer? Justify\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        inputs = qwen_processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "        device = torch.device(\"cuda\")\n",
    "        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        for k in [\"pixel_values\", \"image_grid_thw\", \"video_pixel_values\", \"video_image_size\"]:\n",
    "            if k in inputs:\n",
    "                del inputs[k]\n",
    "    \n",
    "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=5000)\n",
    "        generated_ids_trimmed = [ out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids) ]\n",
    "    \n",
    "        output_text = qwen_processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "    \n",
    "        return output_text\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "751db425-55b3-47af-902f-154a5d5c3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = chat_with_qwen_for_scienceQA(data_sqa[41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f45826f-a964-4622-a424-243cdeb21f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f96adf1-3bac-47a2-9c76-e19f430b6e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "data_vqa = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"validation[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5252236c-8cf2-448b-9fcd-8be82776d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_type': 'what', 'multiple_choice_answer': 'foodiebakercom', 'answers': [{'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 1}, {'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'foodiebaker', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'http://foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'foodiebakercom', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'foodiebaker', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 393225, 'answer_type': 'other', 'question_id': 393225000, 'question': 'What website copyrighted the picture?', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x428 at 0x1553CB5E7750>}\n"
     ]
    }
   ],
   "source": [
    "print(data_vqa[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84ecb271-8a9c-4a33-9fec-31e37b7b6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbec9323-c7d7-4444-9125-cd6799245c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de990cc3-9222-4771-9907-d5bb5608f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_qwen_vqav2(data_point):\n",
    "    image = data_point['image']\n",
    "    if image is None:\n",
    "        return \"No image provided.\"\n",
    "\n",
    "    question = data_point['question']\n",
    "    \n",
    "    # Extract unique answers as choices (e.g., [\"watching\", \"spectating\"])\n",
    "    all_answers = [a[\"answer\"].strip().lower() for a in data_point[\"answers\"]]\n",
    "\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": question + \" Instruction: strictly answer only in a word\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    generated_ids = qwen_model.generate(**inputs, max_new_tokens=1000)\n",
    "    trimmed = [out[len(inp):] for inp, out in zip(inputs[\"input_ids\"], generated_ids)]\n",
    "    output_text = qwen_processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "    pred = output_text.strip().lower()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb812de-4304-419a-a174-e7e19be01375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watching\n"
     ]
    }
   ],
   "source": [
    "output = chat_with_qwen_vqav2(data_vqa[1])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "680e8b16-579b-4129-8044-07f55366f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llava_vqav2(data_point):\n",
    "    image = data_point['image']\n",
    "    if image is None:\n",
    "        return \"No image provided.\"\n",
    "\n",
    "    question = data_point['question']\n",
    "    \n",
    "    all_answers = [a[\"answer\"].strip().lower() for a in data_point[\"answers\"]]\n",
    "\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": question + \" Instruction: strictly answer only in a word\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    inputs = llava_processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    "    ).to(llava_model.device, torch.float16)\n",
    "\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    generate_ids = llava_model.generate(**inputs, max_new_tokens=1000)\n",
    "    trimmed = [out[len(inp):] for inp, out in zip(inputs[\"input_ids\"], generate_ids)]\n",
    "    output_text = llava_processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "    pred = output_text.strip().lower()\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cd2b6b9-1033-4e54-9624-b35491dc8d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watching\n"
     ]
    }
   ],
   "source": [
    "output = chat_with_llava_vqav2(data_vqa[1])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3eabb317-5e34-4d0f-9c68-472586019a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_context(agent_name, data_point):\n",
    "    \"\"\"Initial prompt with image and question\"\"\"\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": data_point[\"image\"]},\n",
    "            {\"type\": \"text\", \"text\": f\"Question: {data_point['question']}\\nPlease explain your answer.\"}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "def build_peer_message(question, peer_responses, image, own_last_response=None):\n",
    "    \"\"\"Builds a user message listing other agents' prior responses and discouraging repetition.\"\"\"\n",
    "    context = \"Other agent responses:\\n\"\n",
    "    for response in peer_responses:\n",
    "        context += f\"\\nOne agent said:\\n{response.strip()}\\n\"\n",
    "\n",
    "    context += f\"\\nThe original question is: {question}\\n\"\n",
    "    if own_last_response:\n",
    "        context += f\"Previously, you answered: '{own_last_response}'. Do you still think that you are correct? Justify your answer and point out why the other agent might be wrong. Do not repeat yourself or simply copy the other agent's answer but give a reasoning.\\n\"\n",
    "\n",
    "    context += \"Please explain your reasoning again.\"\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": context}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response(model, processor, chat_history, image):\n",
    "    \"\"\"Runs chat model with full history\"\"\"\n",
    "    text = processor.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(chat_history)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "    trimmed = [out[len(inp):] for inp, out in zip(inputs[\"input_ids\"], outputs)]\n",
    "    response = processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()\n",
    "    return response\n",
    "\n",
    "def ask_final_answer(model, processor, data_point, prior_response):\n",
    "    \"\"\"Asks for final one-word answer using model and context\"\"\"\n",
    "    prompt = f\"Question: {data_point['question']}\\nEarlier you said:\\n{prior_response}\\n\\nWhat is your final answer? Respond with only one word.\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": data_point[\"image\"]},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    trimmed = [out[len(inp):] for inp, out in zip(inputs[\"input_ids\"], outputs)]\n",
    "    return processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e950bce-ec23-4e60-8275-46502fc0aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqav2_debate(data_point, rounds=3, verbose=True):\n",
    "    qwen_history = build_initial_context(\"Qwen\", data_point)\n",
    "    llava_history = build_initial_context(\"LLaVA\", data_point)\n",
    "    debate_log = []\n",
    "\n",
    "    qwen_prev_response = None\n",
    "    llava_prev_response = None\n",
    "\n",
    "    for r in range(1, rounds + 1):\n",
    "\n",
    "        # Qwen sees LLaVA's prior + discouragement to repeat\n",
    "        if r > 1:\n",
    "            qwen_msg = build_peer_message(\n",
    "                question=data_point[\"question\"],\n",
    "                peer_responses=[llava_prev_response],\n",
    "                image=data_point[\"image\"],\n",
    "                own_last_response=qwen_prev_response\n",
    "            )\n",
    "            qwen_history.append(qwen_msg)\n",
    "\n",
    "        qwen_response = generate_response(qwen_model, qwen_processor, qwen_history, data_point[\"image\"])\n",
    "        qwen_history.append({\"role\": \"assistant\", \"content\": qwen_response})\n",
    "        debate_log.append({\"role\": f\"Qwen (Round {r})\", \"content\": qwen_response})\n",
    "\n",
    "        qwen_prev_response = qwen_response\n",
    "\n",
    "        # LLaVA sees Qwenâ€™s prior + discouragement to repeat\n",
    "        if r > 1:\n",
    "            llava_msg = build_peer_message(\n",
    "                question=data_point[\"question\"],\n",
    "                peer_responses=[qwen_prev_response],\n",
    "                image=data_point[\"image\"],\n",
    "                own_last_response=llava_prev_response\n",
    "            )\n",
    "            llava_history.append(llava_msg)\n",
    "\n",
    "        llava_response = generate_response(llava_model, llava_processor, llava_history, data_point[\"image\"])\n",
    "        llava_history.append({\"role\": \"assistant\", \"content\": llava_response})\n",
    "        debate_log.append({\"role\": f\"LLaVA (Round {r})\", \"content\": llava_response})\n",
    "        llava_prev_response = llava_response\n",
    "\n",
    "        # Ask for final answers (after reasoning)\n",
    "        qwen_final = ask_final_answer(qwen_model, qwen_processor, data_point, qwen_response)\n",
    "        llava_final = ask_final_answer(llava_model, llava_processor, data_point, llava_response)\n",
    "        debate_log.append({\"role\": \"Qwen (Final)\", \"content\": qwen_final})\n",
    "        debate_log.append({\"role\": \"LLaVA (Final)\", \"content\": llava_final})\n",
    "\n",
    "        if qwen_final == llava_final:\n",
    "            debate_log.append({\"role\": \"Consensus\", \"content\": f\"Both agreed on: '{qwen_final}' after {r} round(s).\"})\n",
    "            return {\n",
    "                \"question_id\": data_point[\"question_id\"],\n",
    "                \"multiple_choice_answer\": data_point[\"multiple_choice_answer\"],\n",
    "                \"question\": data_point[\"question\"],\n",
    "                \"image_id\": data_point.get(\"image_id\", None),\n",
    "                \"answer\": qwen_final,\n",
    "                \"rounds\": r,\n",
    "                \"consensus\": True,\n",
    "                \"history\": debate_log\n",
    "            }\n",
    "        \n",
    "    debate_log.append({\"role\": \"Disagreement\", \"content\": f\"No consensus reached after {rounds} rounds.\"})\n",
    "    return {\n",
    "        \"question_id\": data_point[\"question_id\"],\n",
    "        \"multiple_choice_answer\": data_point[\"multiple_choice_answer\"],\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"image_id\": data_point.get(\"image_id\", None),\n",
    "        \"answer\": None,\n",
    "        \"rounds\": None,\n",
    "        \"consensus\": False,\n",
    "        \"history\": debate_log\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7177a4fc-41b9-48ac-850b-4897c3b614f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f0640a1-b95b-444a-941f-1df7a6637e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(data_vqa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6143a30-ddd3-4010-9f6a-2a1af7227b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in data_vqa:\n",
    "    debate_result = vqav2_debate(i)\n",
    "    res.append(debate_result)\n",
    "    if len(res)==250 or len(res)==750:\n",
    "        print(len(res))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c26ccff-bd77-43bd-95da-9f8f3b868169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"vqav2_debate_results2.json\", \"w\") as f:\n",
    "    json.dump(res, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04f98a48-918a-41d1-8465-98f2bf1fcee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 apoorvachavali apoorvachavali 1.3M Jul  4 10:40 vqav2_debate_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "ls -lh vqav2_debate_results.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12af261c-1d21-4ecd-8d03-7321a6ecf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_agree = 0\n",
    "baseline_total = 0\n",
    "baseline_agree_correct = 0\n",
    "baseline_agree_wrong = 0\n",
    "baseline_qwen_correct = 0\n",
    "baseline_llava_correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa5b864f-745b-4de4-8036-c08596edba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_vqa:\n",
    "    a = chat_with_qwen_vqav2(i).strip().lower()\n",
    "    b = chat_with_llava_vqav2(i).strip().lower()\n",
    "    gt = i[\"multiple_choice_answer\"].strip().lower()\n",
    "    \n",
    "    if a == b:\n",
    "        baseline_agree+=1\n",
    "        if a == gt:\n",
    "            baseline_agree_correct+=1\n",
    "        else:\n",
    "            baseline_agree_wrong+=1\n",
    "\n",
    "    if a == gt:\n",
    "        baseline_qwen_correct+=1\n",
    "    if b == gt:\n",
    "        baseline_llava_correct+=1\n",
    "    baseline_total+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d5818a8-41fd-4cf7-8e9d-ea6a351753ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1000\n",
      "Qwen correct: 736 (73.60%)\n",
      "LLaVA correct: 663 (66.30%)\n",
      "Models agreed: 681 (68.10%)\n",
      "Agreement correct: 590 (86.64%)\n",
      "Agreement wrong: 91\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {baseline_total}\")\n",
    "print(f\"Qwen correct: {baseline_qwen_correct} ({baseline_qwen_correct / baseline_total:.2%})\")\n",
    "print(f\"LLaVA correct: {baseline_llava_correct} ({baseline_llava_correct / baseline_total:.2%})\")\n",
    "print(f\"Models agreed: {baseline_agree} ({baseline_agree / baseline_total:.2%})\")\n",
    "print(f\"Agreement correct: {baseline_agree_correct} ({baseline_agree_correct / baseline_agree:.2%})\" if baseline_agree else \"No agreement\")\n",
    "print(f\"Agreement wrong: {baseline_agree_wrong}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29fbb8ed-be79-454f-9fe2-8e1dd1872b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('vqav2_debate_results2.json', 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "abb1afce-f7ae-495c-a8ea-d39acd00862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_llava_agree = 0\n",
    "qwen_llava_agree_correct = 0\n",
    "qwen_llava_agree_wrong = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a1d4cfd-8f53-4d8a-ae44-590bbf67e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 262148001, 'multiple_choice_answer': 'watching', 'question': 'What are the people in the background doing?', 'image_id': 262148, 'answer': 'watching', 'rounds': 1, 'consensus': True, 'history': [{'role': 'Qwen (Round 1)', 'content': 'The people in the background appear to be watching the skateboarder perform a trick. They are standing at a safe distance, observing the action. Some of them might be waiting for their turn or simply enjoying the performance. The presence of onlookers suggests that this is likely a skateboarding event or a casual gathering where people come to watch and support the skateboarders.'}, {'role': 'LLaVA (Round 1)', 'content': \"The people in the background are watching the skateboarder perform his tricks. They are likely observing the skateboarder's skills and enjoying the show.\"}, {'role': 'Qwen (Final)', 'content': 'watching'}, {'role': 'LLaVA (Final)', 'content': 'watching'}, {'role': 'Consensus', 'content': \"Both agreed on: 'watching' after 1 round(s).\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(json_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0a1fabc-004f-4712-a2b0-7270cfc3fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in json_data:\n",
    "    if i['consensus']== True:\n",
    "        qwen_llava_agree+=1\n",
    "        if i['answer'] == i['multiple_choice_answer']:\n",
    "            qwen_llava_agree_correct+=1\n",
    "        else:\n",
    "            qwen_llava_agree_wrong+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a0ce379-dfc5-43a1-8809-4fb517d90c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen and llava agree on  824\n",
      "Qwen and llava agree on and match with GT 627\n",
      "Qwen and llava agree on and does not match with GT 197\n"
     ]
    }
   ],
   "source": [
    "print(\"Qwen and llava agree on \", qwen_llava_agree)\n",
    "print(\"Qwen and llava agree on and match with GT\", qwen_llava_agree_correct)\n",
    "print(\"Qwen and llava agree on and does not match with GT\", qwen_llava_agree_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94c1b06e-8cb0-4d51-ab3a-dd449a7f3369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
